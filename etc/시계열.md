# 시계열 머신러닝·딥러닝 모델링 정리 

## 1. 시계열 데이터란?
- 시간 순서대로 정렬된 데이터 (예: 주식 가격, 날씨, 환율 등)
- 예: 2023-01-01 → 2023-01-02 → 2023-01-03 ...

## 2. 모델링 기본 흐름
1. **데이터 준비**: 시계열 데이터 로드  
2. **정상성 확인**: 패턴이 일정한지 확인  
3. **전처리**: 로그 변환, 차분 등  
4. **데이터 분할**: 훈련/검증/테스트  
5. **모델 학습**: ARIMA, MLP, RNN, LSTM 등  
6. **평가**: MAE, MSE, RMSE 등  
7. **미래 예측**: 향후 데이터 예측  

---

## 3. 기본 개념 정리

### ● 정상성 (Stationarity)
- 평균, 분산이 일정한 데이터
- 예측 모델에 꼭 필요!

### ● 로그 수익률 (Log Return)
```python
log_return = log(today_price / yesterday_price)
```
- 시계열 데이터를 안정적으로 만들어 줌

### ● 차분 (Differencing)
```python
차분 = 오늘 값 - 어제 값
```
- 추세를 없애서 정상성을 만들기 위한 방법

---

## 4. 주요 모델

| 모델 | 특징 | 설명 |
|------|------|------|
| ARIMA | 통계 기반 | p(자기회귀), d(차분), q(이동 평균) 사용 |
| SVR | 머신러닝 | 서포트 벡터로 예측 (과거 데이터를 평면으로 예측) |
| Linear Regression | 간단한 선형 모델 | 시계열 데이터에 직선 피팅 |
| MLP | 다층 퍼셉트론 | 과거 데이터를 받아 여러 층으로 예측 |
| RNN | 순환 신경망 | 시간 흐름을 기억하며 예측 |
| LSTM | 장기 기억 RNN | RNN보다 먼 과거도 기억 가능 |
| GRU | LSTM보다 간단 | 성능 비슷하지만 빠름 |

---

## 5. 평가 지표

| 지표 | 계산법 | 의미 |
|------|--------|------|
| MAE | 평균 절댓값 오차 | 예측이 실제값에서 얼마나 떨어졌는지 |
| MSE | 평균 제곱 오차 | 큰 오차에 더 민감함 |
| RMSE | MSE의 루트 | 단위가 원래 데이터와 같음 |

---

## 6. PyTorch 기반 모델 구성

### ● 입력 구조 이해
- 입력: `[배치크기, 시퀀스길이, 입력특성수]`  
  예: `[64, 7, 1]` → 7일간 1개 값씩 64개 데이터

### ● MLP
```python
self.model = nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, output_size)
)
```

### ● RNN
```python
self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
self.fc = nn.Linear(hidden_size, output_size)
```

### ● LSTM
```python
self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
self.fc = nn.Linear(hidden_size, output_size)
```

---

## 7. RNN 구조 초간단 설명

- RNN은 **순서가 있는 데이터**(시계열)를 다루는 데 특화
- 예측할 때, 이전 입력과 **은닉 상태(hidden state)**를 같이 사용함

```python
out, hidden = rnn(x, hidden)
```

- `x`: 현재 입력  
- `hidden`: 이전 기억  
- `out`: 현재 결과  
- `hidden`: 다음으로 넘길 기억  

---

## 8. LSTM 구조 초간단 설명

- LSTM은 **기억을 오래 유지할 수 있는 RNN**
- 내부에 3개의 게이트 사용:
  - **입력 게이트**: 얼마나 기억할지
  - **망각 게이트**: 얼마나 지울지
  - **출력 게이트**: 어떤 값을 출력할지

---

## 9. 실제 모델 학습 흐름 예시

```python
for epoch in range(epochs):
    model.train()
    for X_batch, y_batch in train_loader:
        output = model(X_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

---

## 10. 예측 결과 복원 방법

> 로그 수익률로 학습했을 경우, 원래 **종가로 복원**하려면 다음과 같이 함:

```python
predicted_price = [start_price]
for log_return in predicted_log_returns:
    next_price = predicted_price[-1] * np.exp(log_return)
    predicted_price.append(next_price)
```

---

## 11. ACF / PACF 간단 설명

- **ACF**: 자기 자신과의 상관관계 (전체)
- **PACF**: 직접적인 상관관계만 고려

| ACF | PACF | 의미 |
|-----|------|------|
| 천천히 감소 | 뚝 끊김 | AR 모델 (자기회귀) |
| 뚝 끊김 | 천천히 감소 | MA 모델 (이동 평균) |
| 둘 다 천천히 감소 |  | 비정상 시계열일 수 있음 |

---

## 12. 추천 학습 순서

1. 시계열 개념 이해 (정상성, 차분, 로그)  
2. ARIMA 구조 이해  
3. 머신러닝 회귀 (SVR, LinearRegression)  
4. 딥러닝 기본 (MLP → RNN → LSTM)  
5. 입력 구조 / 예측 결과 복원 연습  
6. 성능 비교 / 시각화  

---

## 13. 용어 정리

| 용어 | 설명 |
|------|------|
| 배치(batch) | 여러 데이터를 한 번에 학습시키는 단위 |
| 시퀀스 길이 | 예측에 사용하는 과거 일수 |
| 입력 특성 수 | 예: 종가만 있으면 1, 종가+거래량이면 2 |
| hidden state | RNN이 기억하는 상태 |
| epoch | 전체 데이터를 몇 번 반복 학습할지 |
| loss | 예측이 얼마나 틀렸는지를 나타내는 수치 |

---
