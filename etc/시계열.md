# 📊 시계열 예측 
- ARIMA (통계 기반)
- LSTM (딥러닝 기반)
- GRU (딥러닝 기반)
- Transformer (최신 딥러닝 기반)

---

## 1. 시계열이란?

시계열은 **시간의 흐름에 따라 수집된 데이터**다.  

예: 주가, 기온, 환율, 방문자 수 등

- 시간 순서가 중요
  
- 미래를 예측하려면 **과거의 패턴**을 잘 파악해야 한다.

---

## 2. 시계열 데이터에서 검토할 사항

| 항목 | 설명 |
|------|------|
| Trend | 시간에 따라 점점 증가하거나 감소하는 경향 |
| Seasonality | 계절, 요일 등 특정 주기로 반복되는 패턴 |
| Abrupt Change | 갑작스럽고 급격한 변화 |
| Outlier | 다른 값들과 동떨어진 이상치 |
| Constant Variance | 시간에 따라 변동폭이 일정한가? |
| Long-run Cycle | 장기적으로 반복되는 패턴 |
| ACF / PACF | 자기상관성 및 지연된 자기상관성 확인 (모델 파라미터 결정에 사용) |

---

## 3. 모델링 기본 흐름

1. **데이터 준비**: 시계열 데이터 로드  
2. **정상성 확인**: 평균과 분산이 일정한지 확인  
3. **전처리**: 로그 변환, 차분 등  
4. **데이터 분할**: 훈련/검증/테스트 나누기  
5. **모델 학습**: ARIMA, LSTM, GRU, Transformer 등  
6. **모델 평가**: MAE, RMSE 등 사용  
7. **미래 예측**: 향후 데이터 예측  

---

## 4. 주요 용어 정리

| 용어 | 설명 |
|------|------|
| 시계열(Time Series) | 시간 순서대로 정리된 데이터 |
| 예측(Prediction) | 미래 값을 미리 맞추는 것 |
| 정상성(Stationarity) | 평균, 분산이 일정한 상태 |
| 차분(Differencing) | 오늘 - 어제처럼 변화량을 보는 것 |
| ACF | 과거 값과 얼마나 연관 있는지 (자기상관) |
| PACF | 중간 영향 제거 후 순수한 자기상관 |
| MAE | 예측 오차의 평균 |
| 로그 수익률 | log를 사용한 비율 변화 측정 방식 |

---

## 5. 정상성 검정이 필요한 모델들

| 모델 | 정상성 필요 여부 | 설명 |
|------|------------------|------|
| ARIMA | ✅ 반드시 필요 | 비정상일 경우 차분 필요 |
| SARIMA | ✅ 필요 | 계절성 포함된 ARIMA |
| VAR | ✅ 필요 | 여러 시계열을 함께 분석하는 모델 |
| LSTM, GRU, Transformer | ❌ 필요 없음 | 딥러닝 기반이므로 비정상성도 학습 가능 |

---

## 6. 모델 비교표

| 모델 | 특징 | 장점 | 단점 |
|------|------|------|------|
| ARIMA | 수학 기반 통계 모델 | 빠르고 해석 쉬움 | 복잡한 패턴 약함 |
| LSTM | 딥러닝 모델 | 장기 기억 가능 | 느린 학습 |
| GRU | 단순화된 LSTM | 빠른 학습 | 복잡한 기억력 부족 |
| Transformer | 전체 데이터 한 번에 처리 | 병렬 처리, 고성능 | 데이터 많이 필요 |

---

## 7. 실습 예시

### ARIMA 실습

```py
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error

df = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv')
df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)
data = df['Temp']

train = data[:-7]
test = data[-7:]

model = ARIMA(train, order=(5,1,0))
model_fit = model.fit()
forecast = model_fit.forecast(steps=7)

mae = mean_absolute_error(test, forecast)

plt.plot(test.index, test, label='Actual')
plt.plot(test.index, forecast, label='Predicted')
plt.title(f'ARIMA Forecast (MAE: {mae:.2f})')
plt.legend()
plt.show()
```

---

## LSTM 실습 (PyTorch)
```py
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

df = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv')
data = df['Temp'].values.reshape(-1, 1)

scaler = MinMaxScaler()
data = scaler.fit_transform(data)

def create_sequences(data, seq_length):
    x, y = [], []
    for i in range(len(data)-seq_length):
        x.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    return torch.FloatTensor(x), torch.FloatTensor(y)

seq_length = 7
x, y = create_sequences(data, seq_length)

class LSTMModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(1, 64, batch_first=True)
        self.fc = nn.Linear(64, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])

model = LSTMModel()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

for epoch in range(100):
    output = model(x.unsqueeze(-1))
    loss = criterion(output, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

pred = model(x.unsqueeze(-1)).detach().numpy()
pred = scaler.inverse_transform(pred)
true = scaler.inverse_transform(y.numpy().reshape(-1, 1))

plt.plot(true[-50:], label='Actual')
plt.plot(pred[-50:], label='Predicted')
plt.title('LSTM 예측 결과')
plt.legend()
plt.show()
```

---

## 9. GRU 실습 (PyTorch)
```py
class GRUModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.gru = nn.GRU(1, 64, batch_first=True)
        self.fc = nn.Linear(64, 1)

    def forward(self, x):
        out, _ = self.gru(x)
        return self.fc(out[:, -1, :])

model = GRUModel()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

for epoch in range(100):
    output = model(x.unsqueeze(-1))
    loss = criterion(output, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

pred = model(x.unsqueeze(-1)).detach().numpy()
pred = scaler.inverse_transform(pred)
true = scaler.inverse_transform(y.numpy().reshape(-1, 1))

plt.plot(true[-50:], label='Actual')
plt.plot(pred[-50:], label='Predicted')
plt.title('GRU 예측 결과')
plt.legend()
plt.show()
```

---

## 10. Transformer 실습 (PyTorch)
```py
import torch.nn.functional as F

class TransformerModel(nn.Module):
    def __init__(self, seq_length):
        super().__init__()
        self.pos_embedding = nn.Parameter(torch.randn(1, seq_length, 64))
        self.input_linear = nn.Linear(1, 64)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=64, nhead=8),
            num_layers=2
        )
        self.fc = nn.Linear(64, 1)

    def forward(self, x):
        x = self.input_linear(x)
        x = x + self.pos_embedding
        x = self.transformer(x)
        return self.fc(x[:, -1, :])

model = TransformerModel(seq_length)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(100):
    output = model(x.unsqueeze(-1))
    loss = criterion(output, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

pred = model(x.unsqueeze(-1)).detach().numpy()
pred = scaler.inverse_transform(pred)
true = scaler.inverse_transform(y.numpy().reshape(-1, 1))

plt.plot(true[-50:], label='Actual')
plt.plot(pred[-50:], label='Predicted')
plt.title('Transformer 예측 결과')
plt.legend()
plt.show()
```

---

## 요약 

| 모델 | 해석 쉬움 | 복잡한 패턴 학습 | 학습 속도 | 데이터 필요량 |
| --- | --- | --- | --- | --- |
| ARIMA | ✅ | ❌ | ✅ | ❌ |
| LSTM | ❌ | ✅ | ❌ | ✅ |
| GRU | ❌ | ✅ | ✅ | ✅ |
| Transformer | ❌ | ✅✅ | ✅✅ | ✅✅ |

ARIMA → 시계열 기본 개념, 통계

LSTM → 딥러닝 기본

GRU → LSTM보다 빠르게 학습

Transformer → 최신 기술 응용

