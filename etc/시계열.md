# ğŸ“Š ì‹œê³„ì—´ ì˜ˆì¸¡ 
- ARIMA (í†µê³„ ê¸°ë°˜)
- LSTM (ë”¥ëŸ¬ë‹ ê¸°ë°˜)
- GRU (ë”¥ëŸ¬ë‹ ê¸°ë°˜)
- Transformer (ìµœì‹  ë”¥ëŸ¬ë‹ ê¸°ë°˜)

---

## 1. ì‹œê³„ì—´ì´ë€?

ì‹œê³„ì—´ì€ **ì‹œê°„ì˜ íë¦„ì— ë”°ë¼ ìˆ˜ì§‘ëœ ë°ì´í„°**ë‹¤.  

ì˜ˆ: ì£¼ê°€, ê¸°ì˜¨, í™˜ìœ¨, ë°©ë¬¸ì ìˆ˜ ë“±

- ì‹œê°„ ìˆœì„œê°€ ì¤‘ìš”
  
- ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ë ¤ë©´ **ê³¼ê±°ì˜ íŒ¨í„´**ì„ ì˜ íŒŒì•…í•´ì•¼ í•œë‹¤.

---

## 2. ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ê²€í† í•  ì‚¬í•­

| í•­ëª© | ì„¤ëª… |
|------|------|
| Trend | ì‹œê°„ì— ë”°ë¼ ì ì  ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí•˜ëŠ” ê²½í–¥ |
| Seasonality | ê³„ì ˆ, ìš”ì¼ ë“± íŠ¹ì • ì£¼ê¸°ë¡œ ë°˜ë³µë˜ëŠ” íŒ¨í„´ |
| Abrupt Change | ê°‘ì‘ìŠ¤ëŸ½ê³  ê¸‰ê²©í•œ ë³€í™” |
| Outlier | ë‹¤ë¥¸ ê°’ë“¤ê³¼ ë™ë–¨ì–´ì§„ ì´ìƒì¹˜ |
| Constant Variance | ì‹œê°„ì— ë”°ë¼ ë³€ë™í­ì´ ì¼ì •í•œê°€? |
| Long-run Cycle | ì¥ê¸°ì ìœ¼ë¡œ ë°˜ë³µë˜ëŠ” íŒ¨í„´ |
| ACF / PACF | ìê¸°ìƒê´€ì„± ë° ì§€ì—°ëœ ìê¸°ìƒê´€ì„± í™•ì¸ (ëª¨ë¸ íŒŒë¼ë¯¸í„° ê²°ì •ì— ì‚¬ìš©) |

---

## 3. ëª¨ë¸ë§ ê¸°ë³¸ íë¦„

1. **ë°ì´í„° ì¤€ë¹„**: ì‹œê³„ì—´ ë°ì´í„° ë¡œë“œ  
2. **ì •ìƒì„± í™•ì¸**: í‰ê· ê³¼ ë¶„ì‚°ì´ ì¼ì •í•œì§€ í™•ì¸  
3. **ì „ì²˜ë¦¬**: ë¡œê·¸ ë³€í™˜, ì°¨ë¶„ ë“±  
4. **ë°ì´í„° ë¶„í• **: í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë‚˜ëˆ„ê¸°  
5. **ëª¨ë¸ í•™ìŠµ**: ARIMA, LSTM, GRU, Transformer ë“±  
6. **ëª¨ë¸ í‰ê°€**: MAE, RMSE ë“± ì‚¬ìš©  
7. **ë¯¸ë˜ ì˜ˆì¸¡**: í–¥í›„ ë°ì´í„° ì˜ˆì¸¡  

---

## 4. ì£¼ìš” ìš©ì–´ ì •ë¦¬

| ìš©ì–´ | ì„¤ëª… |
|------|------|
| ì‹œê³„ì—´(Time Series) | ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë¦¬ëœ ë°ì´í„° |
| ì˜ˆì¸¡(Prediction) | ë¯¸ë˜ ê°’ì„ ë¯¸ë¦¬ ë§ì¶”ëŠ” ê²ƒ |
| ì •ìƒì„±(Stationarity) | í‰ê· , ë¶„ì‚°ì´ ì¼ì •í•œ ìƒíƒœ |
| ì°¨ë¶„(Differencing) | ì˜¤ëŠ˜ - ì–´ì œì²˜ëŸ¼ ë³€í™”ëŸ‰ì„ ë³´ëŠ” ê²ƒ |
| ACF | ê³¼ê±° ê°’ê³¼ ì–¼ë§ˆë‚˜ ì—°ê´€ ìˆëŠ”ì§€ (ìê¸°ìƒê´€) |
| PACF | ì¤‘ê°„ ì˜í–¥ ì œê±° í›„ ìˆœìˆ˜í•œ ìê¸°ìƒê´€ |
| MAE | ì˜ˆì¸¡ ì˜¤ì°¨ì˜ í‰ê·  |
| ë¡œê·¸ ìˆ˜ìµë¥  | logë¥¼ ì‚¬ìš©í•œ ë¹„ìœ¨ ë³€í™” ì¸¡ì • ë°©ì‹ |

---

## 5. ì •ìƒì„± ê²€ì •ì´ í•„ìš”í•œ ëª¨ë¸ë“¤

| ëª¨ë¸ | ì •ìƒì„± í•„ìš” ì—¬ë¶€ | ì„¤ëª… |
|------|------------------|------|
| ARIMA | âœ… ë°˜ë“œì‹œ í•„ìš” | ë¹„ì •ìƒì¼ ê²½ìš° ì°¨ë¶„ í•„ìš” |
| SARIMA | âœ… í•„ìš” | ê³„ì ˆì„± í¬í•¨ëœ ARIMA |
| VAR | âœ… í•„ìš” | ì—¬ëŸ¬ ì‹œê³„ì—´ì„ í•¨ê»˜ ë¶„ì„í•˜ëŠ” ëª¨ë¸ |
| LSTM, GRU, Transformer | âŒ í•„ìš” ì—†ìŒ | ë”¥ëŸ¬ë‹ ê¸°ë°˜ì´ë¯€ë¡œ ë¹„ì •ìƒì„±ë„ í•™ìŠµ ê°€ëŠ¥ |

---

## 6. ëª¨ë¸ ë¹„êµí‘œ

| ëª¨ë¸ | íŠ¹ì§• | ì¥ì  | ë‹¨ì  |
|------|------|------|------|
| ARIMA | ìˆ˜í•™ ê¸°ë°˜ í†µê³„ ëª¨ë¸ | ë¹ ë¥´ê³  í•´ì„ ì‰¬ì›€ | ë³µì¡í•œ íŒ¨í„´ ì•½í•¨ |
| LSTM | ë”¥ëŸ¬ë‹ ëª¨ë¸ | ì¥ê¸° ê¸°ì–µ ê°€ëŠ¥ | ëŠë¦° í•™ìŠµ |
| GRU | ë‹¨ìˆœí™”ëœ LSTM | ë¹ ë¥¸ í•™ìŠµ | ë³µì¡í•œ ê¸°ì–µë ¥ ë¶€ì¡± |
| Transformer | ì „ì²´ ë°ì´í„° í•œ ë²ˆì— ì²˜ë¦¬ | ë³‘ë ¬ ì²˜ë¦¬, ê³ ì„±ëŠ¥ | ë°ì´í„° ë§ì´ í•„ìš” |

---

## 7. ì‹¤ìŠµ ì˜ˆì‹œ

### ARIMA ì‹¤ìŠµ

```py
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error

df = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv')
df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)
data = df['Temp']

train = data[:-7]
test = data[-7:]

model = ARIMA(train, order=(5,1,0))
model_fit = model.fit()
forecast = model_fit.forecast(steps=7)

mae = mean_absolute_error(test, forecast)

plt.plot(test.index, test, label='Actual')
plt.plot(test.index, forecast, label='Predicted')
plt.title(f'ARIMA Forecast (MAE: {mae:.2f})')
plt.legend()
plt.show()
```

---

## LSTM ì‹¤ìŠµ (PyTorch)
```py
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

df = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv')
data = df['Temp'].values.reshape(-1, 1)

scaler = MinMaxScaler()
data = scaler.fit_transform(data)

def create_sequences(data, seq_length):
    x, y = [], []
    for i in range(len(data)-seq_length):
        x.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    return torch.FloatTensor(x), torch.FloatTensor(y)

seq_length = 7
x, y = create_sequences(data, seq_length)

class LSTMModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(1, 64, batch_first=True)
        self.fc = nn.Linear(64, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])

model = LSTMModel()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

for epoch in range(100):
    output = model(x.unsqueeze(-1))
    loss = criterion(output, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

pred = model(x.unsqueeze(-1)).detach().numpy()
pred = scaler.inverse_transform(pred)
true = scaler.inverse_transform(y.numpy().reshape(-1, 1))

plt.plot(true[-50:], label='Actual')
plt.plot(pred[-50:], label='Predicted')
plt.title('LSTM ì˜ˆì¸¡ ê²°ê³¼')
plt.legend()
plt.show()
```

---

## 9. GRU ì‹¤ìŠµ (PyTorch)
```py
class GRUModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.gru = nn.GRU(1, 64, batch_first=True)
        self.fc = nn.Linear(64, 1)

    def forward(self, x):
        out, _ = self.gru(x)
        return self.fc(out[:, -1, :])

model = GRUModel()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

for epoch in range(100):
    output = model(x.unsqueeze(-1))
    loss = criterion(output, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

pred = model(x.unsqueeze(-1)).detach().numpy()
pred = scaler.inverse_transform(pred)
true = scaler.inverse_transform(y.numpy().reshape(-1, 1))

plt.plot(true[-50:], label='Actual')
plt.plot(pred[-50:], label='Predicted')
plt.title('GRU ì˜ˆì¸¡ ê²°ê³¼')
plt.legend()
plt.show()
```

---

## 10. Transformer ì‹¤ìŠµ (PyTorch)
```py
import torch.nn.functional as F

class TransformerModel(nn.Module):
    def __init__(self, seq_length):
        super().__init__()
        self.pos_embedding = nn.Parameter(torch.randn(1, seq_length, 64))
        self.input_linear = nn.Linear(1, 64)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=64, nhead=8),
            num_layers=2
        )
        self.fc = nn.Linear(64, 1)

    def forward(self, x):
        x = self.input_linear(x)
        x = x + self.pos_embedding
        x = self.transformer(x)
        return self.fc(x[:, -1, :])

model = TransformerModel(seq_length)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(100):
    output = model(x.unsqueeze(-1))
    loss = criterion(output, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

pred = model(x.unsqueeze(-1)).detach().numpy()
pred = scaler.inverse_transform(pred)
true = scaler.inverse_transform(y.numpy().reshape(-1, 1))

plt.plot(true[-50:], label='Actual')
plt.plot(pred[-50:], label='Predicted')
plt.title('Transformer ì˜ˆì¸¡ ê²°ê³¼')
plt.legend()
plt.show()
```

---

## ìš”ì•½ 

| ëª¨ë¸ | í•´ì„ ì‰¬ì›€ | ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ | í•™ìŠµ ì†ë„ | ë°ì´í„° í•„ìš”ëŸ‰ |
| --- | --- | --- | --- | --- |
| ARIMA | âœ… | âŒ | âœ… | âŒ |
| LSTM | âŒ | âœ… | âŒ | âœ… |
| GRU | âŒ | âœ… | âœ… | âœ… |
| Transformer | âŒ | âœ…âœ… | âœ…âœ… | âœ…âœ… |

ARIMA â†’ ì‹œê³„ì—´ ê¸°ë³¸ ê°œë…, í†µê³„

LSTM â†’ ë”¥ëŸ¬ë‹ ê¸°ë³¸

GRU â†’ LSTMë³´ë‹¤ ë¹ ë¥´ê²Œ í•™ìŠµ

Transformer â†’ ìµœì‹  ê¸°ìˆ  ì‘ìš©

