## 1. ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ê³¼ ë”¥ëŸ¬ë‹(Deep Learning)ì˜ ì°¨ì´

| êµ¬ë¶„ | ë¨¸ì‹ ëŸ¬ë‹ | ë”¥ëŸ¬ë‹ |
|------|-----------|------------|
| ì •ì˜ | ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ ì°¾ì•„ ì˜ˆì¸¡í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ | ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì´ìš©í•œ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ |
| ì…ë ¥ ë°ì´í„° | ìˆ˜ì‘ì—… íŠ¹ì§• ì¶”ì¶œ í•„ìš” | ì›ì‹œ ë°ì´í„°ë¥¼ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥ |
| ëª¨ë¸ ë³µì¡ë„ | ë‚®ìŒ~ì¤‘ê°„ | ë§¤ìš° ë†’ìŒ (ìˆ˜ë§ì€ íŒŒë¼ë¯¸í„°) |
| í•™ìŠµ ì†ë„ | ë¹ ë¦„ | ëŠë¦¼ (ë§ì€ ì—°ì‚°ëŸ‰ í•„ìš”) |
| ëŒ€í‘œ ì•Œê³ ë¦¬ì¦˜ | SVM, KNN, ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ë“± | CNN, RNN, LSTM, Transformer ë“± |

targetì€ predictionì´ ë§ì¶°ì•¼ í•  ì •ë‹µì´ê³  epochì€ í•™ìŠµì˜ íšŸìˆ˜ë¥¼ ê°€ë¦¬í‚¨ë‹¤.

<br><br><br>

---

## 2. ë¨¸ì‹ ëŸ¬ë‹ ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜

### ì§€ë„í•™ìŠµ(Supervised Learning)
- ë ˆì´ë¸”(ì •ë‹µ)ì´ ìˆëŠ” ë°ì´í„°ë¥¼ í•™ìŠµ
- ì˜ˆì‹œ: ë¶„ë¥˜(Classification), íšŒê·€(Regression)
  - ìŠ¤íŒ¸ ì´ë©”ì¼ ë¶„ë¥˜, ì§‘ê°’ ì˜ˆì¸¡

  
**ì£¼ìš” ì•Œê³ ë¦¬ì¦˜:**
- ì„ í˜• íšŒê·€(Linear Regression)
- ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Tree)
- ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forest)
- SVM(Support Vector Machine)

<br><br>

### ë¹„ì§€ë„í•™ìŠµ(Unsupervised Learning)
- ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ë¥¼ êµ°ì§‘í™” ë˜ëŠ” íŠ¹ì§• ì¶”ì¶œ(ì •ë‹µì´ ì—†ëŠ” ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ ì°¾ê¸°)   
- ex.ê³ ê° êµ°ì§‘í™”, ë°ì´í„° ì°¨ì› ì¶•ì†Œ

**ì£¼ìš” ì•Œê³ ë¦¬ì¦˜:**
- K-Means
- PCA (ì£¼ì„±ë¶„ ë¶„ì„)
- ê³„ì¸µì  êµ°ì§‘(Hierarchical Clustering)

<br><br>

### ê°•í™”í•™ìŠµ(Reinforcement Learning)
- í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ë³´ìƒì„ ìµœëŒ€í™”í•˜ë„ë¡ í•™ìŠµí•˜ëŠ” ë°©ì‹
- **íŠ¹ì§•**: í–‰ë™ â†’ ë³´ìƒ â†’ ì •ì±… í•™ìŠµ ë°©ì‹

**ì£¼ìš” ê°œë…:**
- Agent (í–‰ìœ„ì)
- Environment (í™˜ê²½)
- Action (í–‰ë™)
- Reward (ë³´ìƒ)

<br><br><br>

---

## 3. ë”¥ëŸ¬ë‹ ê¸°ë³¸ ìš©ì–´

| ìš©ì–´ | ì„¤ëª… |
|------|------|
| Epoch | ì „ì²´ í•™ìŠµ ë°ì´í„°ë¥¼ ëª‡ ë²ˆ ë°˜ë³µ í•™ìŠµí•  ê²ƒì¸ì§€ |
| Batch Size | í•œ ë²ˆì— í•™ìŠµí•˜ëŠ” ë°ì´í„° ìƒ˜í”Œ ìˆ˜ |
| Iteration | í•œ Epoch ë™ì•ˆ ë°˜ë³µ íšŸìˆ˜ (ë°ì´í„° ìˆ˜ / ë°°ì¹˜ í¬ê¸°) |
| Loss | ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ (ì˜¤ì°¨) |
| Optimizer | ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ê°±ì‹ í•´ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ë„êµ¬ (Adam, SGD ë“±) |
| Learning Rate | ê°€ì¤‘ì¹˜ë¥¼ ì–¼ë§ˆë‚˜ í¬ê²Œ ì¡°ì •í• ì§€ ê²°ì •í•˜ëŠ” ê°’ |
| Overfitting | í•™ìŠµ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ì í•©ë˜ì–´ ì¼ë°˜í™”ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ |
| Underfitting | í•™ìŠµ ë°ì´í„°ì¡°ì°¨ ì˜ ì˜ˆì¸¡í•˜ì§€ ëª»í•˜ëŠ” ìƒíƒœ |

<br><br><br>

---

## 4. ì†ì‹¤ í•¨ìˆ˜(Loss Function)

| ì¢…ë¥˜ | ì„¤ëª… |
|------|------|
| MSE (Mean Squared Error) | í‰ê·  ì œê³± ì˜¤ì°¨, íšŒê·€ì—ì„œ ì£¼ë¡œ ì‚¬ìš© |
| MAE (Mean Absolute Error) | í‰ê·  ì ˆëŒ€ ì˜¤ì°¨, ì´ìƒì¹˜ì— ëœ ë¯¼ê° |
| Cross Entropy | ë¶„ë¥˜ ë¬¸ì œì—ì„œ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ |

<br><br><br>

---

## 5. í™œì„±í™” í•¨ìˆ˜(Activation Function)

| í•¨ìˆ˜ | ì„¤ëª… |
|------|------|
| ReLU | 0ë³´ë‹¤ ì‘ìœ¼ë©´ 0, í¬ë©´ ê·¸ëŒ€ë¡œ ì¶œë ¥ (ê°€ì¥ ë§ì´ ì‚¬ìš©ë¨) |
| Sigmoid | ì¶œë ¥ì´ 0~1 ì‚¬ì´ (ì´ì§„ ë¶„ë¥˜ì— ì‚¬ìš©) |
| Tanh | ì¶œë ¥ì´ -1~1 ì‚¬ì´ |
| Softmax | ì¶œë ¥ê°’ì„ í™•ë¥ ë¡œ ë³€í™˜ (ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜) |

```py
model = Sequential([
    # 1ì°¨ì› feature map ìƒì„±
    Conv1D(filters=32, kernel_size=5,
           padding="causal",
           activation="relu",
           input_shape=[WINDOW_SIZE, 1]),
    # LSTM
    LSTM(16, activation='tanh'),
    Dense(16, activation="relu"),
    Dense(1),
])
```

[ì°¸ê³  ì½”ë“œ](https://github.com/haedal-uni/analysis/blob/main/work/2025-04/250408_LSTM%EC%9D%84_%ED%99%9C%EC%9A%A9%ED%95%9C_%EC%A3%BC%EA%B0%80_%EC%98%88%EC%B8%A1_%EB%AA%A8%EB%8D%B8.ipynb)

<br><br><br>

---

## 6. ì‹ ê²½ë§ ê¸°ë³¸ êµ¬ì¡°

### 1) í¼ì…‰íŠ¸ë¡ (Perceptron)
- ì¸ê³µ ë‰´ëŸ° í•œ ê°œë¥¼ ëª¨ë¸ë§í•œ ê²ƒ
- ì…ë ¥ * ê°€ì¤‘ì¹˜ â†’ ì´í•© â†’ í™œì„±í™” í•¨ìˆ˜ â†’ ì¶œë ¥

<br><br>

### 2) MLP (ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ )
```py
self.model = nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, output_size)
)
```

<br><br><br>

---

## 7. ëª¨ë¸ í›ˆë ¨ ê¸°ë³¸ ì½”ë“œ íë¦„
```py
for epoch in range(epochs):
    model.train()
    for X_batch, y_batch in train_loader:
        output = model(X_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

<br><br><br>

---

## 8. ìì£¼ ë“±ì¥í•˜ëŠ” ìš©ì–´ ì •ë¦¬

| ìš©ì–´ | ì„¤ëª… |
|------|------|
| Feature | ì…ë ¥ ë³€ìˆ˜ (ì„¤ëª… ë³€ìˆ˜) |
| Label | ì¶œë ¥ ë³€ìˆ˜ (ëª©í‘œ ê°’) |
| Weight | ê° ì…ë ¥ì— ê³±í•´ì§€ëŠ” ê°€ì¤‘ì¹˜ |
| Bias | ì¶œë ¥ì— ë”í•´ì§€ëŠ” ê°’ (í¸í–¥) |
| Backpropagation | ì˜¤ì°¨ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” ë°©ë²• |
| Gradient Descent | ì†ì‹¤ ìµœì†Œí™”ë¥¼ ìœ„í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ |

<br><br><br>

---

## 9. ë°ì´í„° ì „ì²˜ë¦¬ 

| ê°œë… | ì„¤ëª… |
|------|------|
| ì •ê·œí™”(Normalization) | ë°ì´í„°ì˜ ë²”ìœ„ë¥¼ [0,1]ë¡œ ì¡°ì • |
| í‘œì¤€í™”(Standardization) | í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ì¡°ì • |
| One-hot encoding | ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì´ì§„ ë²¡í„°ë¡œ ë³€í™˜ |
| ê²°ì¸¡ê°’ ì²˜ë¦¬ | í‰ê· /ì¤‘ì•™ê°’ ëŒ€ì²´, ì œê±°, ì˜ˆì¸¡ ë“± |

<br><br><br>

---

## 10. ì¶”ì²œ í•™ìŠµ ìˆœì„œ

1. ë¨¸ì‹ ëŸ¬ë‹ì˜ ê°œë…ê³¼ ì§€ë„/ë¹„ì§€ë„ í•™ìŠµ ì´í•´
2. ëŒ€í‘œì ì¸ ëª¨ë¸(SVM, KNN, Decision Tree) ì‹¤ìŠµ
3. ì˜¤ì°¨ì™€ í‰ê°€ ì§€í‘œ ì´í•´ (MSE, MAE, Accuracy ë“±)
4. ë”¥ëŸ¬ë‹ êµ¬ì¡° (Perceptron â†’ MLP â†’ CNN â†’ RNN) ìµíˆê¸°
5. PyTorch ë˜ëŠ” TensorFlow ê¸°ë³¸ ë¬¸ë²• ì—°ìŠµ
6. í•™ìŠµ íë¦„, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì—­í•  íŒŒì•…
7. ë°ì´í„° ì „ì²˜ë¦¬ ë° ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹¤ìŠµ

<br><br><br>

---

## 11. ë°ì´í„° ë¶„í•  ì „ëµ ë° ì˜ˆì œ ì½”ë“œ

### 11.1 `train_test_split()` ì‚¬ìš©
```py
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```
*20%ë¥¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©

ë˜ëŠ” Numpyë¡œ ì§ì ‘ ë¶„í• í•˜ê±°ë‚˜ StratifiedKFold/GroupKFold ì‚¬ìš© ê°€ëŠ¥

<br><br>

### 11.2 Numpyë¡œ ì§ì ‘ ë‚˜ëˆ„ê¸° (í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸)
```py
import numpy as np

# ë°ì´í„° ì„ê¸°
indices = np.arange(len(X))
np.random.seed(42)
np.random.shuffle(indices)

# ë¹„ìœ¨ ì„¤ì • (60% í•™ìŠµ, 20% ê²€ì¦, 20% í…ŒìŠ¤íŠ¸)
train_end = int(0.6 * len(X))
val_end = int(0.8 * len(X))

X_train, y_train = X[indices[:train_end]], y[indices[:train_end]]
X_val, y_val = X[indices[train_end:val_end]], y[indices[train_end:val_end]]
X_test, y_test = X[indices[val_end:]], y[indices[val_end:]]
```

<br><br>

### 11.3 `StratifiedKFold` (ê³„ì¸µì  ë¶„í• )
```py
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5)
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
```

<br><br>

### 11.4 `GroupKFold` (ê·¸ë£¹ ê¸°ë°˜ ë¶„í• )
```py
from sklearn.model_selection import GroupKFold

groups = ...  # ì˜ˆ: ì‚¬ìš©ì ID ë“±

gkf = GroupKFold(n_splits=5)
for train_idx, test_idx in gkf.split(X, y, groups):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
```

<br><br><br>

---

## 12. ëŒ€í‘œì ì¸ ì´ë¯¸ì§€ ë°ì´í„°ì…‹

### 12.1 MNIST (ìˆ«ì ì†ê¸€ì”¨)
```py
from keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
```
`mnist.load_data()` : MNIST ë°ì´í„°ì…‹ì€ 60000ê°œì˜ í›ˆë ¨ ë°ì´í„°ì™€ 10000ê°œì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ê³ ì •ë˜ì–´ ì œê³µ

<br><br>

### 12.2 Fashion-MNIST (ì˜ë¥˜ ì´ë¯¸ì§€)
```py
from keras.datasets import fashion_mnist
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
```

<br><br>

### 12.3 CIFAR-10 (ì»¬ëŸ¬ ì´ë¯¸ì§€)
```py
from keras.datasets import cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
```

<br><br>

### 12.4 PyTorchìš© TensorDataset & DataLoader ì˜ˆì‹œ
```py
from torch.utils.data import TensorDataset, DataLoader

dataset = TensorDataset(torch.tensor(X), torch.tensor(y))
data_loader = DataLoader(dataset, batch_size=64, shuffle=True)
```

<br><br><br>

---


## 13. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Hyperparameter Tuning)

**í•˜ì´í¼íŒŒë¼ë¯¸í„°ë€?**
- í•™ìŠµ ì „ì— ì‚¬ìš©ìê°€ ì§€ì •í•˜ëŠ” ê°’ (ì˜ˆ: learning rate, batch size, epoch ìˆ˜)

**íŠœë‹ ë°©ë²•:**
- **Grid Search**: ë¯¸ë¦¬ ì •ì˜í•œ ê°’ë“¤ì„ ì¡°í•©ìœ¼ë¡œ íƒìƒ‰
- **Random Search**: ë¬´ì‘ìœ„ ì¡°í•©ìœ¼ë¡œ íƒìƒ‰
- **Optuna / Hyperopt**: ë² ì´ì§€ì•ˆ ìµœì í™” ê¸°ë°˜ ìë™í™” ë„êµ¬

```py
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

params = {'n_estimators': [100, 200], 'max_depth': [5, 10]}
grid = GridSearchCV(RandomForestClassifier(), param_grid=params, cv=3)
grid.fit(X_train, y_train)
print(grid.best_params_)
```

---

## 14. íŒŒì¸ íŠœë‹ (Fine-Tuning)

**ì •ì˜:**
- ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸(pretrained model)ì˜ ì¼ë¶€ ë˜ëŠ” ì „ì²´ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì— ë§ì¶”ëŠ” ë°©ë²•

**ì˜ˆì‹œ:**
- ImageNetìœ¼ë¡œ í•™ìŠµëœ VGG16 ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì ¸ì™€, ë§ˆì§€ë§‰ layerë§Œ êµì²´í•´ ì¬í•™ìŠµ

```py
from tensorflow.keras.applications import VGG16

base_model = VGG16(weights='imagenet', include_top=False)
for layer in base_model.layers:
    layer.trainable = False  # ê¸°ì¡´ ê°€ì¤‘ì¹˜ ê³ ì •

# ìƒˆë¡­ê²Œ ë¶„ë¥˜ ë ˆì´ì–´ ì¶”ê°€
model = Sequential([
    base_model,
    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])
```

---

## 15. ìœˆë„ìš° ìŠ¬ë¼ì´ì‹± (Windowing)ê³¼ ë¡¤ë§ (Rolling)

### 15.1 ìœˆë„ìš°(Windowing)
- ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì¼ì •í•œ í¬ê¸°ì˜ êµ¬ê°„(window)ìœ¼ë¡œ ì˜ë¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
- ì˜ˆ: ê³¼ê±° 7ì¼ ë°ì´í„°ë¥¼ ì´ìš©í•´ ì˜¤ëŠ˜ ê°’ì„ ì˜ˆì¸¡

```py
WINDOW_SIZE = 7
X, y = [], []
for i in range(len(data) - WINDOW_SIZE):
    X.append(data[i:i+WINDOW_SIZE])
    y.append(data[i+WINDOW_SIZE])
```

### 15.2 ë¡¤ë§(Rolling)
- ì‹œê°„ì¶•ì„ ë”°ë¼ ì´ë™í•˜ë©° í†µê³„ê°’(í‰ê· , í‘œì¤€í¸ì°¨ ë“±) ê³„ì‚°

```py
import pandas as pd

df['rolling_mean'] = df['price'].rolling(window=7).mean()
df['rolling_std'] = df['price'].rolling(window=7).std()
```

---

## 16. ë”¥ëŸ¬ë‹ ëª¨ë¸ ì˜ˆì œ (Conv1D + LSTM)

```py
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, LSTM, Dense

model = Sequential([
    Conv1D(filters=32, kernel_size=5, padding="causal", activation="relu", input_shape=[WINDOW_SIZE, 1]),
    LSTM(16, activation='tanh'),
    Dense(16, activation="relu"),
    Dense(1),
])
```

### ì½”ë“œ í•´ì„:
- **Conv1D**: ì‹œê³„ì—´ì˜ ë¡œì»¬ íŒ¨í„´(ë³€í™”ëŸ‰)ì„ ë¨¼ì € ì¶”ì¶œ
- **LSTM**: ì‹œê°„ì˜ íë¦„ì„ ë°˜ì˜í•´ ê¸°ì–µí•˜ê³  ì˜ˆì¸¡
- **Dense**: ìµœì¢… ì¶œë ¥ê°’ ê³„ì‚° (íšŒê·€ ë¬¸ì œì—ì„œ ì‚¬ìš©)

---

## 17. PyTorch MLP ì½”ë“œ

```py
import torch.nn as nn

self.model = nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, output_size)
)
```

- **input_size**: ì…ë ¥ ë²¡í„° í¬ê¸°
- **hidden_size**: ì€ë‹‰ì¸µ ë…¸ë“œ ìˆ˜
- **output_size**: ì˜ˆì¸¡ê°’ í¬ê¸° (1ì´ë©´ íšŒê·€)

---

## 18. ëª¨ë¸ í•™ìŠµ ë£¨í”„

```py
for epoch in range(epochs):
    model.train()
    for X_batch, y_batch in train_loader:
        output = model(X_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

- **loss.backward()**: ì˜¤ì°¨ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê¸°ìš¸ê¸° ê³„ì‚°
- **optimizer.step()**: íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
- **optimizer.zero_grad()**: ì´ì „ ê¸°ìš¸ê¸° ì´ˆê¸°í™”


---

## ê¸°íƒ€ ì •ë¦¬

- **ê³¼ì í•©(Overfitting)** : ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì—ë§Œ ë„ˆë¬´ ì˜ ë§ì¶”ê³  ìƒˆë¡œìš´ ë°ì´í„°ì—ëŠ” ì„±ëŠ¥ì´ ë‚®ì€ ìƒíƒœ
- **ì†ì‹¤(Loss)** : ì •ë‹µê³¼ ì˜ˆì¸¡ê°’ ê°„ì˜ ê±°ë¦¬ ë˜ëŠ” ì°¨ì´ë¥¼ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•œ ê°’ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **ì§€ë„í•™ìŠµ** : ì •ë‹µì´ ìˆëŠ” ë°ì´í„°ë¥¼ ì´ìš©í•´ í•™ìŠµí•˜ëŠ” ë°©ë²•. ë¶„ë¥˜/íšŒê·€ ë¬¸ì œì—ì„œ ì‚¬ìš©
- **ë¹„ì§€ë„í•™ìŠµ** : ì •ë‹µ ì—†ì´ ë°ì´í„°ì˜ êµ¬ì¡°ë‚˜ íŒ¨í„´ì„ í•™ìŠµ (êµ°ì§‘í™”, ì°¨ì› ì¶•ì†Œ ë“±)
- **ê°•í™”í•™ìŠµ** : ë³´ìƒì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ í–‰ë™ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•. ê²Œì„, ë¡œë´‡ ë“±ì— ë§ì´ ì‚¬ìš©


---

## 13. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Hyperparameter Tuning)

**í•˜ì´í¼íŒŒë¼ë¯¸í„°ë€?**

ëª¨ë¸ì´ í•™ìŠµë˜ê¸° ì „ì— ì‚¬ëŒì´ ì§ì ‘ ì„¤ì •í•´ì£¼ëŠ” ê°’ì…ë‹ˆë‹¤.

ì˜ˆ: í•™ìŠµë¥ (Learning Rate), ë°°ì¹˜ í¬ê¸°(Batch Size), ì€ë‹‰ì¸µ í¬ê¸°, ì—í¬í¬ ìˆ˜ ë“±

| í•˜ì´í¼íŒŒë¼ë¯¸í„° | ì„¤ëª… |
| --- | --- |
| Learning Rate | ê°€ì¤‘ì¹˜ë¥¼ ì–¼ë§ˆë‚˜ í¬ê²Œ ì—…ë°ì´íŠ¸í• ì§€ ê²°ì • |
| Epochs | ì „ì²´ ë°ì´í„°ë¥¼ ëª‡ ë²ˆ ë°˜ë³µí•  ê²ƒì¸ì§€ |
| Batch Size | í•œ ë²ˆì— í•™ìŠµí•  ë°ì´í„° ìƒ˜í”Œ ìˆ˜ |
| Hidden Units | ì€ë‹‰ì¸µ ë‰´ëŸ° ìˆ˜ (ëª¨ë¸ì˜ ë³µì¡ë„ ê²°ì •) |
| Dropout Rate | ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë¬´ì‘ìœ„ ë…¸ë“œ ë¹„í™œì„±í™” ë¹„ìœ¨ |

### 13.1 ê·¸ë¦¬ë“œ ì„œì¹˜ (GridSearchCV)
```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20]
}

grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)
grid_search.fit(X_train, y_train)

print("ìµœì  íŒŒë¼ë¯¸í„°:", grid_search.best_params_)

```

- ì—¬ëŸ¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì‹œë„í•´ë³´ê³  ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ì°¾ìŒ

<br>

### 13.2 ëœë¤ ì„œì¹˜ (RandomizedSearchCV)

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_dist = {
    'n_estimators': randint(50, 200),
    'max_depth': [None, 10, 20, 30]
}

random_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_dist, n_iter=10, cv=3)
random_search.fit(X_train, y_train)

print("ìµœì  íŒŒë¼ë¯¸í„°:", random_search.best_params_)
```

- GridSearchë³´ë‹¤ ë¹ ë¥´ê²Œ íŠœë‹ ê°€ëŠ¥ (ì¼ë¶€ ì¡°í•©ë§Œ ì‹œë„)

<br><br><br>

## 14. ëª¨ë¸ í‰ê°€ ì§€í‘œ

| í‰ê°€ ì§€í‘œ | ì„¤ëª… | ì‚¬ìš© ì˜ˆì‹œ |
| --- | --- | --- |
| Accuracy | ì „ì²´ ì˜ˆì¸¡ ì¤‘ ë§ì¶˜ ë¹„ìœ¨ | ë¶„ë¥˜(Classification) |
| Precision | ì˜ˆì¸¡ ì¤‘ ì§„ì§œ ì •ë‹µì¸ ë¹„ìœ¨ | ìŠ¤íŒ¸ í•„í„°ì—ì„œ ì¤‘ìš” |
| Recall | ì‹¤ì œ ì •ë‹µ ì¤‘ ì–¼ë§ˆë‚˜ ë§ì·„ëŠ”ì§€ | ì•” ì§„ë‹¨ ë“± ë¯¼ê°í•œ ê²½ìš° |
| F1 Score | ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê·  | ë¶ˆê· í˜• ë°ì´í„° |
| MSE | ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì œê³± í‰ê·  | íšŒê·€(Regression) |
| MAE | ì ˆëŒ€ ì˜¤ì°¨ í‰ê·  | íšŒê·€, ì´ìƒì¹˜ì— ëœ ë¯¼ê° |

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"ì •í™•ë„: {accuracy}, ì •ë°€ë„: {precision}, ì¬í˜„ìœ¨: {recall}, F1: {f1}")

```

<br><br><br>

## 15. PyTorch ê¸°ë³¸ í•™ìŠµ ì½”ë“œ í…œí”Œë¦¿

### 15.1 ëª¨ë¸ ì •ì˜

```python
import torch
import torch.nn as nn

# ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (MLP)
class MLPModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLPModel, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x):
        return self.model(x)

```

### 15.2 í•™ìŠµ/ê²€ì¦ ì½”ë“œ íë¦„

```python
model = MLPModel(input_dim=10, hidden_dim=32, output_dim=1)
criterion = nn.MSELoss()  # ì†ì‹¤ í•¨ìˆ˜
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(50):  # ì´ 50ë²ˆ ë°˜ë³µ
    model.train()  # í•™ìŠµ ëª¨ë“œ ì„¤ì •
    for X_batch, y_batch in train_loader:
        y_pred = model(X_batch)  # ì˜ˆì¸¡
        loss = criterion(y_pred, y_batch)  # ì†ì‹¤ ê³„ì‚°
        optimizer.zero_grad()  # ê¸°ì¡´ ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        loss.backward()  # ì—­ì „íŒŒ
        optimizer.step()  # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

    print(f"Epoch {epoch+1} - Loss: {loss.item():.4f}")

```

> ğŸ’¡ train_loaderëŠ” í•™ìŠµ ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¶ì–´ì£¼ëŠ” ì—­í• 

<br><br><br>

---

## 16. ë”¥ëŸ¬ë‹ ì£¼ìš” êµ¬ì¡° ìš”ì•½

| êµ¬ì¡° | ì„¤ëª… | ì‚¬ìš© ì˜ˆì‹œ |
| --- | --- | --- |
| MLP | ì…ë ¥ â†’ ì€ë‹‰ì¸µ â†’ ì¶œë ¥ì¸µ | ê¸°ì´ˆ íšŒê·€, ë¶„ë¥˜ ë¬¸ì œ |
| CNN | ì´ë¯¸ì§€ ì²˜ë¦¬, í•„í„° í™œìš© | ì´ë¯¸ì§€ ë¶„ë¥˜ (MNIST ë“±) |
| RNN | ì‹œê³„ì—´ ë°ì´í„° ìˆœì„œ ìœ ì§€ | ì£¼ê°€ ì˜ˆì¸¡, ìì—°ì–´ ì²˜ë¦¬ |
| LSTM | ì¥ê¸° ê¸°ì–µ ìœ ì§€ê°€ ê°€ëŠ¥í•œ RNN | ë‰´ìŠ¤ ê°ì„± ë¶„ì„ |
| GRU | LSTMë³´ë‹¤ êµ¬ì¡° ë‹¨ìˆœí•œ RNN | ì‹œê³„ì—´ ì˜ˆì¸¡ |
| Transformer | ë³‘ë ¬ì²˜ë¦¬ ê°•ì , ì–´í…ì…˜ ê¸°ë°˜ | ë²ˆì—­, ì±—ë´‡, BERT |

<br><br><br>

---

## 17. ì‹œê³„ì—´ ì˜ˆì¸¡ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì˜ˆì‹œ (LSTM)

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•œ LSTM ëª¨ë¸ êµ¬ì„±
model = Sequential([
    LSTM(50, activation='tanh', input_shape=(10, 1)),  # 10ì¼ì¹˜ ë°ì´í„°ë¥¼ ì…ë ¥
    Dense(1)  # ë‹¤ìŒ ë‚  ê°’ì„ ì˜ˆì¸¡
])

model.compile(loss='mse', optimizer='adam')
model.summary()
```

`input_shape=(10, 1)`ì€ 10ì¼ì¹˜ ë°ì´í„°ë¥¼ ì‹œê³„ì—´ ìˆœì„œëŒ€ë¡œ 1ì°¨ì›ìœ¼ë¡œ ì…ë ¥í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

## 18. ìš©ì–´ ì‹œê° ì˜ˆì‹œ (í•œëˆˆì— ì´í•´)

```
[Feature]   [Weight]    [Sum]   [Activation]   [Output]
   x1     â†’    w1     â†’    +     â†’   ReLU     â†’    y1
   x2     â†’    w2     â†’    +     â†’   ReLU     â†’    y2
```

- ê° ì…ë ¥ xì— ê°€ì¤‘ì¹˜ wë¥¼ ê³±í•˜ê³ , í•©ì„ êµ¬í•´ í™œì„±í™” í•¨ìˆ˜ë¡œ ì „ë‹¬ â†’ ì¶œë ¥ ìƒì„±

## 19. ë§ˆë¬´ë¦¬ ìš”ì•½

- **ë¨¸ì‹ ëŸ¬ë‹**ì€ ê·œì¹™ì„ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜, **ë”¥ëŸ¬ë‹**ì€ ì‹ ê²½ë§ ê¸°ë°˜ì˜ ë³µì¡í•œ ëª¨ë¸
- ì§€ë„/ë¹„ì§€ë„/ê°•í™”í•™ìŠµìœ¼ë¡œ êµ¬ë¶„
- ëª¨ë¸ êµ¬ì¡°, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, í‰ê°€ ì§€í‘œ ë“±ì€ ì‹¤ë¬´ì˜ í•µì‹¬
- PyTorchì™€ TensorFlowëŠ” ë”¥ëŸ¬ë‹ì„ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ëŒ€í‘œ í”„ë ˆì„ì›Œí¬
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ ì„±ëŠ¥ ê°œì„  ê°€ëŠ¥
- ì „ì²˜ë¦¬ â†’ í•™ìŠµ â†’ í‰ê°€ â†’ íŠœë‹ â†’ ë°°í¬ ìˆœìœ¼ë¡œ ëª¨ë¸ë§ ì§„í–‰
